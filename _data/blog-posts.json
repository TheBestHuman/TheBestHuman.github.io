[
  {
    "id": "684b23a89d1d6d0001047191",
    "uuid": "d8f4aa5f-3461-4f05-a79b-890a92934169",
    "title": "I Wrote My Dream App in 4 Hours",
    "slug": "i-wrote-my-dream-app-in-4-hours",
    "html": "<p>Since the release of GPT 3.5 (almost 3 years ago!) I’ve had a few app ideas that I’ve used as a benchmark for how good LLMs are at coding. It’s a list of projects that I test each major model and tool breakthroughs against, and if I can build one of these projects, it represents a true advancement in the field. With GitHub Copilot Agent Mode, I’ve finally been able to build the first app on my list, <a href=\"https://melticulous.com/?ref=pr.ogra.ms\" rel=\"noreferrer\">Melticulous</a>.</p><p>My entire family is obsessed with <a href=\"https://en.wikipedia.org/wiki/Fuse_beads?ref=pr.ogra.ms\" rel=\"noreferrer\">Perler Beads</a> (generic name: fuse beads.) It’s a deceptively simple craft where you arrange small beads on a grid of pegs and then fuse them together with a hot iron. You can create really elaborate designs with them, and they’re really good for making pixel art.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://pr.ogra.ms/content/images/2025/06/IMG_8937.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"868\" srcset=\"https://pr.ogra.ms/content/images/size/w600/2025/06/IMG_8937.png 600w, https://pr.ogra.ms/content/images/size/w1000/2025/06/IMG_8937.png 1000w, https://pr.ogra.ms/content/images/size/w1600/2025/06/IMG_8937.png 1600w, https://pr.ogra.ms/content/images/size/w2400/2025/06/IMG_8937.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">A perler bead pattern of Nintendo's Mario</span></figcaption></figure><p>I‘ve long wanted to create an app that turns arbitrary images into Perler bead patterns. I think it would be really neat to be able to make hangable art based on photos of your kids, or landscapes you took on your dream vacation.</p><h3 id=\"proof-of-concept\">Proof of Concept</h3><p>I’ve been trying to build this app on every LLM model since 2022, with a slow progression of results, but I would always get stuck on the <a href=\"https://en.wikipedia.org/wiki/Color_quantization?ref=pr.ogra.ms\" rel=\"noreferrer\">color quantization</a> algorithm. That is - the algorithm for choosing which colors should be part of the palette. I wanted to limit the number of colors to 16, because who (besides my wife) has a warehouse full of beads? But many images have millions of different colors and I know nothing about the math involved with reducing the number of colors in an image. This is what I needed LLMs for; somehow magically figuring out the math involved in quantizing the colors in a way that felt correct to the user. With every model, I was hitting a brick wall at this stage. </p><p>That is, until VSCode (GitHub Copilot) added agent mode and increased their context windows back in <a href=\"https://code.visualstudio.com/blogs/2025/04/07/agentMode?ref=pr.ogra.ms\" rel=\"noreferrer\">April</a>.  I tried prompting it on a whim using the free mode with GPT-4o and to my surprise, it only took a few tweaks to get a working proof of concept! Here's my experience taking it from PoC to a solid, usable app, with some tips along the way.</p><h3 id=\"how-does-it-work-just-ask\">How does it work? Just ask!</h3><p>I showed a few people the PoC and got some feedback, but the biggest question software developers asked was \"How does it work?\" I knew that the key breakthrough was to ask it to choose the most unalike colors to be part of the palette, but I didn't know <em>how</em> it made that choice. So I just asked!</p><blockquote>generate a document for me that gives me enough information about the app to discuss it with other devs. it should include enough detail about the tech decisions made to be able to discuss in technical company. take note of interesting algorithms like the min-max color quantization and explain how they work to someone that knows math but has forgotten a lot of it :)</blockquote><p>It built me a document that explained everything in simple language, including a section on color quantization:</p><p><em><strong>Min-Max (Max-Min) Color Quantization</strong><br>To reduce the number of bead colors while preserving image diversity, the app uses a greedy max-min algorithm:</em></p><ol><li><em><strong>Start</strong> with the most common color in the image as the first palette entry.</em></li><li><em><strong>Iteratively add</strong> the color that is farthest (in Euclidean RGB space) from all colors already in the palette.</em></li><li><em><strong>Repeat </strong>until the palette reaches the desired size (e.g., 16 colors) or all unique colors are included.</em></li><li><em><strong>Pad</strong> with white if needed.</em></li></ol><p><em>This ensures the palette covers the full range of colors in the image, not just the most frequent ones, and avoids clustering similar colors.</em></p><h3 id=\"agents-can%E2%80%99t-see\">Agents Can’t See</h3><p>In order for the app to be useful, it needed to generate very precise patterns that fit under the plastic fuse-bead peg boards exactly. If the grid measurements are off by even a millimeter, it can mess up the whole image.<em> </em>I spent a while trying to iterate on the layout with Copilot, but ultimately I had to physically print out the results and hold them up to a peg board to see if it was printing in alignment.</p><p>My first try was way off, even though the measurements were to the correct fuse bead pattern specifications. I needed to iterate, but I realized that printing out a new sheet every time I made a small tweak would have been a huge waste of time and paper, so I had to find another way to test the alignment.</p><p>Luckily, there are tons of free printable fuse bead patterns online, so in order to streamline the process, I downloaded a free PDF and measured all of the allowances in Adobe Acrobat. I fed all of that into Copilot and then took the output and overlayed it on the printable pattern I had downloaded using the excellent Photoshop clone, <a href=\"https://www.photopea.com/?ref=pr.ogra.ms\" rel=\"noreferrer\">Photopea</a>. This gave me a way tighter (and less wasteful) feedback loop and I was able to get a very precise output. After the overlay was exact, I printed out a physical copy to verify it aligned with a real peg board.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://pr.ogra.ms/content/images/2025/06/Greenshot-2025-06-20-17.10.35.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1184\" height=\"414\" srcset=\"https://pr.ogra.ms/content/images/size/w600/2025/06/Greenshot-2025-06-20-17.10.35.png 600w, https://pr.ogra.ms/content/images/size/w1000/2025/06/Greenshot-2025-06-20-17.10.35.png 1000w, https://pr.ogra.ms/content/images/2025/06/Greenshot-2025-06-20-17.10.35.png 1184w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">My results overlayed on the original fuse-bead pattern @ 44% opacity</span></figcaption></figure><h3 id=\"sometimes-its-smarter-than-you\">Sometimes it's Smarter Than You</h3><p>It was around this part of the process that I realized that Copilot had done something smart that I hadn't thought of. When I was conceptualizing this app, I assumed it would need a backend API because processing the image would take a long time. But the LLM built all of the processing into the browser, and the UI was actually really snappy. This design simplified a lot of things from my end: I don't have to maintain a compute infrastructure and manage access. This can be hosted directly on a CDN for fast download, and performance is limited only by the user’s own device.</p><p>But the biggest advantage is that because <a href=\"https://melticulous.com/?ref=pr.ogra.ms\" rel=\"noreferrer\">Melticulous</a> runs in the browser, it's completely secure and users don't have to worry about me storing their files, EVER.</p><h3 id=\"the-9090-rule\">The 90/90 Rule</h3><p>All of these early positive results were really encouraging, but the devil is in the details and <a href=\"https://en.wikipedia.org/wiki/Ninety%E2%80%93ninety_rule?ref=pr.ogra.ms\" rel=\"noreferrer\">the 90/90 rule</a> started to set in.</p><blockquote>The first 90 percent of the code accounts for the first 90 percent of the development time. The remaining 10 percent of the code accounts for the other 90 percent of the development time.</blockquote><p>I'm not sure that I can fully pin this on Copilot, as this rule has been around for a long time, but there was a fair amount of whack-a-mole at this stage. I would try to add some UI Calls-to-Action and the grid would be misaligned. Or, I would try to change the wording of something and the buttons would look weird.</p><h3 id=\"use-git-and-commit-often\">Use git and Commit Often</h3><p>I have long had a policy of not writing a stitch of code on any project without doing <em>git init</em> first. It's an excellent habit, and it saved me more than a few times while building this project.</p><p>I was at the point where the app was looking really polished and I decided I needed to add an about page. I had Copilot generate the page and was making some tweaks to the wording when Copilot went rogue and replaced the entire app with a large spinning logo. I loaded up the page after a \"wording edit\" and there was no app anymore, it had reached into the folder and grabbed an image that I wasn't using for anything and made it huge and rotating.</p><p>Then Copilot acted like everything was fine. I tried to argue a bit, and I even tried stepping backward using the built-in functionality, but the app was destroyed at that point. I reverted to a known good version in git and tried again. Don't even think about vibe coding without git. You need it. I'm serious.</p><h3 id=\"test-whats-important-to-you\">Test What's Important to You</h3><p>By the same token, make sure to add unit tests for all of the core functionality that's important to you. To me, the cell alignment of the printed pattern was the most important immutable aspect of this application. So I had Copilot generate a unit test and tested the unit test by purposely screwing up the alignment, forcing the test to fail. </p><p>Unit tests alone aren't enough unless you run them regularly. I asked Copilot to add a step in the <a href=\"https://about.gitlab.com/?ref=pr.ogra.ms\" rel=\"noreferrer\">GitLab</a> Pipeline to run my unit tests before any merge to main.</p><h3 id=\"sometimes-its-just-not-the-agent%E2%80%99s-day\">Sometimes, It's Just Not The Agent’s Day</h3><p>The last bit of caution I'd like to leave people with is that sometimes Copilot, like any developer, gets stuck trying to solve a problem a certain way. You need to be on top of this and realize when it's stuck in a loop trying to do something that will never work. In this situation, you have two options, which you can employ separately or together:</p><ol><li>Start the context over: ask Copilot to summarize the conversation and the work that it's done and store it to a file. Then erase your history and re-describe the problem. Sometimes restating the problem a different way to a brand new instance is enough to nudge Copilot in the right direction.</li><li>Switch models: This may not be a popular opinion, but most of the modern models are almost the same in terms of their abilities. Some really thoughtful tasks require a reasoning model, but other than that, they're pretty interchangeable. So interchange them - don't tie yourself to a given model, think of changing models as a way to change perspective.</li></ol><h3 id=\"conclusion\">Conclusion</h3><p>If you're a software developer, you need to be learning these tools. At this point, it's not optional. Agents aren't going to replace human software engineers, but human engineers that aren't using these tools run the risk of being replaced by human engineers that do.</p><p>If you’re into crafting with Perler Beads, don’t forget to take <a href=\"https://melticulous.com/?ref=pr.ogra.ms\" rel=\"noreferrer\">Melticulous</a> for a spin; and if you have any enhancement ideas, <a href=\"mailto:kimmelb+melticulous@gmail.com\" rel=\"noreferrer\">let me know</a>!</p><p></p>",
    "comment_id": "684b23a89d1d6d0001047191",
    "feature_image": "https://pr.ogra.ms/content/images/2025/06/IMG_0392.jpeg",
    "featured": true,
    "visibility": "public",
    "created_at": "2025-06-12T14:59:52.000-04:00",
    "updated_at": "2025-06-23T22:17:51.000-04:00",
    "published_at": "2025-06-23T17:10:00.000-04:00",
    "custom_excerpt": "Using Copilot Agent Mode, I was able to implement an app idea that I’ve been mulling for years. ",
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "custom_template": null,
    "canonical_url": null,
    "url": "https://pr.ogra.ms/i-wrote-my-dream-app-in-4-hours/",
    "excerpt": "Using Copilot Agent Mode, I was able to implement an app idea that I’ve been mulling for years. ",
    "reading_time": 7,
    "access": true,
    "comments": false,
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "email_subject": null,
    "frontmatter": null,
    "feature_image_alt": null,
    "feature_image_caption": "<a href=\"https://melticulous.com\" rel=\"noreferrer\"><span style=\"white-space: pre-wrap;\">Melticulous</span></a><span style=\"white-space: pre-wrap;\">, an app that turns your images into Perler Bead Patterns</span>"
  },
  {
    "id": "67b093799b59fc0001cb92cd",
    "uuid": "3056a793-9582-4074-900a-eef4f09e9e7d",
    "title": "The LLM Plateau and What Software Engineers Can Do About It",
    "slug": "the-llm-plateau-and-what-to-do-about-it-2",
    "html": "<p>When was the last time a new LLM blew you away with its level of intelligence? Is OpenAI’s o3 mini really any better than o1? Is DeepSeek really any worse?</p><p>There have been great innovations in the past year in efficiency and features, but the models aren’t getting much smarter. We’re charging towards a Large Language Model plateau.</p><p>OpenAI’s recent <a href=\"https://arstechnica.com/ai/2025/02/sam-altman-lays-out-roadmap-for-openais-long-awaited-gpt-5-model/?ref=pr.ogra.ms\" rel=\"noreferrer\">announcement</a> of their roadmap for GPT 4.5/5 is a strong signal of the coming plateau. Along with the announcement of 4.5 within “weeks” and  5 coming soon was the news that users will no longer be able to choose which model will be used to answer this question. Reading deeper into the announcement tells us 2 things:</p><ol><li>GPT-5 isn’t ready, but OpenAI felt the need to progress, so they’re releasing a new model as GPT-4.5</li><li>OpenAI is betting that most users can’t differentiate between the models, so they’re implementing a UI change that will reduce costs.</li></ol><p>Both of these imply that there aren’t any major intelligence leaps on the horizon, at least from OpenAI. </p><h2 id=\"so-what%E2%80%99s-next\">So what’s next?</h2><p>All of this is not to say that innovation is going to slow or stop, but that the new frontier in LLMs is not going to be in AGI or superintelligence. Advances over the next few years are going to focus on training efficiency, cheaper implementations, and UI advancements.</p><p>DeepSeek has already started making strides in training efficiency with <a href=\"https://www.vellum.ai/blog/the-training-of-deepseek-r1-and-ways-to-use-it?ref=pr.ogra.ms\" rel=\"noreferrer\">GRPO</a>. This is only the beginning, and plans to spend $500 billion on hardware are either ridiculous folly or intentional grift. </p><p>Implementation costs are already plummeting and the raft of competing models is only going to pressure the market to process more request faster, and crucially, increase context windows so that LLMs can respond to more and more complex questions. </p><p>Finally, people are already starting to realize that chat is not always the best interface for AI. Tools like Microsoft CoPilot, Cursor, and Windsurf are going to get better and better at assisting Engineers in their day-to-day tasks. </p><p>How Can We as Software Engineers Prepare?</p><p>We’re in an AI bubble right now. Companies talking about reducing their workforce due to AI are either too bullish or masking layoffs that they were going to make anyway. At some point, the bubble will burst and there will be a great need for engineers again. But we’ll need to know how to efficiently leverage these new tools.</p><p>Anyone who’s tried to use an LLM for any project bigger than a few files will know that they’re bad at maintaining a consistent design. Even with larger context windows, models like to rewrite irrelevant parts of the code and have recency bias towards solving the current problem and ignoring the big picture.</p><p>The good news is, this forces some best practice that we should be doing anyway; namely the Single Responsibility Principle and Test-Driven Development. LLMs are good at working on small bits of testable code. Writing code this way allows us to take better advantage of LLMs along with all of the existing positives of TDD.</p><p>This leaves design and architecture up to us humans. This is an area where we’re still way better than AI. We can consider things like extensibility, readability, performance, and scalability all at once, then break the project into small testable units for the LLM to write. </p><p>These skills will be essential in a world where AI models stagnate, with the added bonus of being good practice. </p><p>LLMs are also very good at reading code, and there are more ways to leverage that than you think.</p><p>AI code reviews: feed your coding standards document along with an output of the changes into an LLM and ask it to code review. As context windows increase in size, you’ll eventually be able to feed your whole codebase in and ask for unintentional side effects. </p><p>AI documentation: LLMs are also great for summarizing large pieces of code, and for documenting APIs. When context windows get large enough, they'll be able to extract and diagram your code’s design. You could even regenerate this documentation continuously so that it’s never out of date. </p><p>AI user stories: This may be controversial, but LLMs can help to automate the most time consuming part of writing user stories- converting your natural language descriptions into acceptance criteria. Taking a general description and converting it into a detailed set of requirements that can then be reviewed for accuracy can help reduce requirements gaps and help engineers work of the correct implementation. </p><p>Conclusion</p><p>Please remember that <em>I’m a sign, not a cop</em>. I’m speculating based on my experiences with LLMs and having lived through many different bubble/bust cycles over the past 30 years, but nobody can predict the future… except maybe a super intelligent AI. </p>",
    "comment_id": "67b093799b59fc0001cb92cd",
    "feature_image": null,
    "featured": false,
    "visibility": "public",
    "created_at": "2025-02-15T08:15:37.000-05:00",
    "updated_at": "2025-02-16T10:04:34.000-05:00",
    "published_at": "2025-02-16T10:04:34.000-05:00",
    "custom_excerpt": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "custom_template": null,
    "canonical_url": null,
    "url": "https://pr.ogra.ms/the-llm-plateau-and-what-to-do-about-it-2/",
    "excerpt": "When was the last time a new LLM blew you away with its level of intelligence? Is OpenAI’s o3 mini really any better than o1? Is DeepSeek really any worse?\n\nThere have been great innovations in the past year in efficiency and features, but the models aren’t getting much smarter. We’re charging towards a Large Language Model plateau.\n\nOpenAI’s recent announcement of their roadmap for GPT 4.5/5 is a strong signal of the coming plateau. Along with the announcement of 4.5 within “weeks” and 5 coming",
    "reading_time": 3,
    "access": true,
    "comments": false,
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "email_subject": null,
    "frontmatter": null,
    "feature_image_alt": null,
    "feature_image_caption": null
  },
  {
    "id": "66576ed0cad2e0000138bed1",
    "uuid": "31604698-fc99-445c-b6ae-05bf0c376185",
    "title": "The Three Continuums of Better Software Estimation",
    "slug": "tips-for-better-software-estimation",
    "html": "<p>As a software engineer, you're frequently asked to estimate how long a task will take to complete. Estimation can be a tedious process and it's tempting to just throw out a (usually rosy) date. This approach can quickly get you and your team buried, working nights and weekends to heroically meet dates that were unrealistic to begin with. Better estimates are critical to your health and longevity as a software engineer and this framing can help you keep your workload sustainable, avoid burnout, and as a side effect, give your audience better estimates.</p><h2 id=\"define-better\">Define \"Better\"</h2><p>It's easy to define \"better\" here, right? A better estimate is one where the estimated date provided before the work starts matches the real date that the work was delivered. But what if the team works 80 hour weeks to meet their deadlines and needs a week of recovery time after every production release? Or the opposite; what if the team spent 1 month estimating 3 months of work and hit their date exactly? \"Better\" doesn't always mean \"more accurate.\" A good estimate will <strong><em>give the organization the information it needs to coordinate and make decisions most efficiently.</em></strong></p><p>When a product manager asks how long something will take, they're usually trying to answer a specific question, such as: </p><ul><li><em>When can we tell a client this feature will be done?</em></li><li><em>How does this project affect another team that is dependent on this API to release the new version of their product?</em></li><li><em>Where can I put this project on our publicly available roadmap?</em></li><li><em>How close are we to hitting our quarterly targets?</em></li></ul><p>Surprisingly, <strong>the best possible estimate</strong> for each of the above scenarios<strong> can be different, <em>even for the same body of work</em></strong>.</p><p>Estimating work is just like any project. You're used to defining requirements, asking questions, being clear about the definition of done, etc. when you're building things. You should follow the same process when tasked with estimating a body of work. There are a few questions that you can ask to help you figure out your targets for each of the 3 continuums:</p><p><strong>Who is the audience for this estimate? </strong>The estimate may be for internal projections, internal teams, or it may be widely distributed to clients (or all three.) </p><p><strong>What are the consequences of over- or under-estimating this work?</strong> If you overestimate, will you miss out on a valuable contract? If you underestimate, will you end up working overtime to catch up, only to have the project ultimately cancelled? Do other teams depend on the timing of delivery of this work?</p><p><strong>To what level of detail is this work specified?</strong> Are we t-shirt sizing for a long-term project or is there enough information to break the work down into tasks and assign hours to each task?</p><p>Getting answers to these three questions will give you valuable insight into where on the three continuums of <strong>effort, optimism, </strong>and <strong>certainty</strong> this estimate should fall.</p><h2 id=\"effort\">Effort</h2><p>The amount of effort you put into an estimate can increase your accuracy, up to a point. For almost every estimation exercise, however, there is a point of diminishing returns. The two extreme approaches of this continuum are:</p><ol><li>Respond with an estimate as soon as possible (off-the-cuff); this approach can lead to inaccurate estimates.</li><li>Try to get a full and complete understanding of the work, only then providing an estimate; this approach seems \"correct\" at first, but can actually imply false certainty, which is also inaccurate.</li></ol><p>Finding the point of diminishing returns here is key to providing the right estimate.</p><p>If the work has detailed specifications and the estimate is being used directly in a high-profile contract with a client, it makes sense to take more time to analyze all of the requirements.</p><p>If you're setting quarterly targets, and you only have high-level requirements, discussing minutiae is not an efficient use of time. To be clear, the minutiae and requirements will be discussed (during sprint planning, for example,) but answering every question may not be required for high-level estimates.</p><p>Whether you're estimating the work by yourself or with your team, it's important to focus on details <em>only if the outcome of the discussion would affect the estimate</em>. Try not to mix up requirements communication with estimation, especially when you're estimating at a high level. </p><h2 id=\"optimism\">Optimism</h2><p>Between <a href=\"https://www.standishgroup.com/sample_research_files/CHAOSReport2015-Final.pdf?ref=pr.ogra.ms\" rel=\"noreferrer\">20% and 66% of software projects fail</a>, depending on your criteria for failure. At first glance, you might say that as an industry, we are critically failing in our ability to estimate software projects. And you'd be right, but why?</p><p>One factor that contributes to our failure to estimate well is the <em>natural tendency to give optimistic estimates. </em>Pressure from stakeholders, lack of knowledge of the requirements, personal pride, and <strong><em>the inherent uncertainty in trying to predict the future</em></strong>, all contribute to a culture where software engineers are set up to give rosy estimates.</p><p>So that's an easy problem to fix, right? Just pad all estimates by 30% and you should be good. This works and has worked for years in resource-rich environments. However, in an environment where your teams, proposals, and governance are competing for resources with other teams, and those teams are providing optimistic estimates with no padding, you'll be starved for funding.</p><p>This is why it's crucial to understand your audience and the consequences of over or under estimating for the particular body of work that you're estimating. <em>There are situations where it's appropriate to sacrifice accuracy for optimism and your estimate should be optimistic, and there are situations where going over time or budget is unacceptable.</em></p><p>There are two big ways that you can control your level of optimism when providing an estimate: feedback loops and padding</p><h3 id=\"feedback-loops\">Feedback loops</h3><p>Feedback loops just mean that you have some mechanism for checking your previous estimates against reality, with an eye towards increasing accuracy. </p><p>Retrospectives are a great place to have this discussion with the rest of your team. Make sure that there's time to talk through how much was estimated vs. how much was done during your retrospectives so that you can continuously improve your accuracy. </p><p>A small note: many teams have a formalized feedback loop for task- or story-level estimations, but don't actively try to improve their longer-term estimates. Having retrospectives per-release, per-quarter and beyond can help you plan further into the future (to the extent to which that's possible!)</p><h3 id=\"padding\">Padding</h3><p>Padding is an age-old method for ensuring that your projects don't run over-time or over-budget. It used to be conventional wisdom that each level of management would add 20-30% to every estimate before communicating it out. This method works, but it has a few key drawbacks.</p><p>The first is that it's a very imprecise tool. That could be ok in certain situations where you absolutely, positively must deliver on time. It's impreciseness has the side effect that it's one-size fits all and doesn't scale well. The magnitude of padding on a long-term project with a high degree of uncertainty may be totally different than the padding on a small, well-specified project.</p><p>The second drawback is <a href=\"https://en.wikipedia.org/wiki/Parkinson%27s_law?ref=pr.ogra.ms\" rel=\"noreferrer\">Parkinson's law</a>, <em>\"work expands so as to fill the time available for its completion.\"</em> There is some evidence that this law applies to software engineering and sticking with a defined strategy for estimation, rather than adding a flat padding percent can help mitigate Parkinson's law.</p><p>I would recommend that you use padding sparingly, and when you do, use a defined and transparent methodology that ties the magnitude of padding to level of risk.</p><h2 id=\"certainty\">Certainty</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://pr.ogra.ms/content/images/2024/06/Cone_of_Uncertainty.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"960\" height=\"720\" srcset=\"https://pr.ogra.ms/content/images/size/w600/2024/06/Cone_of_Uncertainty.jpg 600w, https://pr.ogra.ms/content/images/2024/06/Cone_of_Uncertainty.jpg 960w\" sizes=\"(min-width: 720px) 720px\"></figure><p>If you haven't already, stop reading this and go read <a href=\"https://www.amazon.com/Software-Estimation-Demystifying-Developer-Practices/dp/0735605351?ref=pr.ogra.ms\" rel=\"noreferrer\">Steve McConnell's Software Estimation: Demystifying the Black Art</a>. At the heart of this book is a diagram called <strong>The Cone of Uncertainty</strong>. The concept is simple: the earlier in the project you're estimating a body of work, the less accurate the estimate will be.</p><p>It's important to note that the uncertainty does not come from the skill or abilities of the people estimating. <strong>There is nothing you can do to remove the uncertainty without spending time. </strong>Whether you spend that time articulating requirements or iterating, even the most accurate possible estimator can't do better than a .25-4x range at the very beginning of a project.</p><p>We have two ways of incorporating uncertainty into our estimates that can be used in concert: We can improve the specifications, and/or accept and communicate the level of uncertainty.</p><h3 id=\"improve-the-specifications\">Improve the Specifications</h3><p>Even at a high level, there should be some definition of done. The key is to have the appropriate definition of done for the specificity of the estimate. For example, if you're estimating building a complex new page, it's not important to have pixel-perfect mockups for a high-level estimation. Make sure that <em>at a minimum, you have answers to any question that would affect the estimate.</em></p><p>Sometimes you can't get answers to every question you have about the requirements. For that situation, you're going to have to make an assumption about the answer. When you make specification assumptions in order to provide an estimate, make sure to <strong><em>clearly document those assumptions.</em></strong></p><p>As for how to decide on which assumption to choose, ask the person writing requirements (Product Manager, SME, Business Liaison, etc.,) to take their best guess. If they won't venture a guess, you can always choose the option that would take the most time. The key is to document that as an assumption.</p><h3 id=\"acceptcommunicate-the-level-of-uncertainty\">Accept/Communicate the Level of Uncertainty</h3><p>There are a few ways to accept and communicate the level of uncertainty in your estimates. You can include a certainty percentage along with a time, or provide a range of dates rather than a hard deadline. I've found that the game of telephone can strip this extra risk information out of your estimates. The person or team to whom you are providing the estimate may intentionally or unintentionally omit the  percentage or choose a date within the range when they are passing on the information to another party.</p><p>Another way to communicate the level of uncertainty is to codify that in your process. Repeated usage of different types of estimates can acclimate the whole organization to your level of uncertainty for different types of estimates. For this, it helps to give names to different uncertainty levels for different types of work and ensure there is a shared understanding of what they mean.</p><p>A popular taxonomy (from most uncertainty to least uncertainty) is <strong>Initiative, Epic, Story, Task</strong>. Some organizations also use <strong>Theme</strong> and <strong>Feature</strong>. For estimation, it's more important that there's a shared understanding of what level of uncertainty each of these maps to than the actual words you use to describe those levels.</p><h2 id=\"the-key-to-better-estimates\">The Key to \"Better\" estimates</h2><p>The key to better estimates is to understand that, just like all software deliverables, the \"why\" matters just as much as the \"what\" does. The task of estimation itself has requirements that you need to understand. When you find out more about what each estimate will be used for, you can tweak the sliders on each of the three continuums of <strong>effort, optimism, and certainty</strong> to provide a customized result that <strong><em>above all, meets the requirements of the estimate itself.</em></strong></p><p></p><h2 id=\"bonus-estimation-advice\">Bonus Estimation Advice</h2><p><em>You </em>may understand the above, and be working toward clarifying a shared understanding within your organization, but not everyone will exactly align on this framework. For that, I have a few helpful tips:</p><p><strong><em>Never provide off-the-cuff estimates</em></strong> If someone tries to pressure you into estimating something over the phone without having any requirements in writing and without giving you time to think, I advise you not to give an estimate if at all possible. Asking for off-the-cuff estimates and pressuring software engineers for a quick answer is a way to codify and enforce unrealistic timelines, leading to burnout. If someone can't wait for you to have an amount of time with the requirements that's appropriate for the level of uncertainty, then they are usually trying to trap you into giving a rosy estimate and reserving the right for future feature creep. Protect your mental and physical health and well-being by having clear boundaries about this.</p><p><strong><em>Only estimate work that you or your team will be doing</em></strong>. Estimates are not transferrable, you can't tell someone how long it will take another person to complete a software development task. If you are a leader or manager, don't estimate tasks that your team will be implementing, bring them into the estimation process. Part of the estimation process is a commitment from the estimator to aim for the date provided (given all assumptions and caveats mentioned above.) This commitment gives the people doing the work accountability and buy-in for the estimate. You can't make that commitment for another person or team.</p><p></p>",
    "comment_id": "66576ed0cad2e0000138bed1",
    "feature_image": "https://pr.ogra.ms/content/images/2024/05/Estimation-Sliders.jpg",
    "featured": false,
    "visibility": "public",
    "created_at": "2024-05-29T14:07:12.000-04:00",
    "updated_at": "2024-06-02T18:18:21.000-04:00",
    "published_at": "2024-06-02T18:18:21.000-04:00",
    "custom_excerpt": null,
    "codeinjection_head": null,
    "codeinjection_foot": null,
    "custom_template": null,
    "canonical_url": null,
    "url": "https://pr.ogra.ms/tips-for-better-software-estimation/",
    "excerpt": "As a software engineer, you're frequently asked to estimate how long a task will take to complete. Estimation can be a tedious process and it's tempting to just throw out a (usually rosy) date. This approach can quickly get you and your team buried, working nights and weekends to heroically meet dates that were unrealistic to begin with. Better estimates are critical to your health and longevity as a software engineer and this framing can help you keep your workload sustainable, avoid burnout, a",
    "reading_time": 8,
    "access": true,
    "comments": false,
    "og_image": null,
    "og_title": null,
    "og_description": null,
    "twitter_image": null,
    "twitter_title": null,
    "twitter_description": null,
    "meta_title": null,
    "meta_description": null,
    "email_subject": null,
    "frontmatter": null,
    "feature_image_alt": "Three futuristic-looking slider bars stacked on top of each other with the labels, in order: EFFORT, OPTIMISM, and CERTAINTY",
    "feature_image_caption": null
  }
]